{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly Data Science Immersive -- Elliot Cohen\n",
    "## Part 1\n",
    "### Python Coding and Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the data file and header file provided\n",
    "header_url = 'https://gist.githubusercontent.com/jeff-boykin/b5c536467c30d66ab97cd1f5c9a3497d/raw/5233c792af49c9b78f20c35d5cd729e1307a7df7/field_names.txt'\n",
    "header_list = pd.read_csv(header_url, header=None, squeeze=True).tolist();\n",
    "\n",
    "data_url = 'https://gist.githubusercontent.com/jeff-boykin/b5c536467c30d66ab97cd1f5c9a3497d/raw/5233c792af49c9b78f20c35d5cd729e1307a7df7/breast-cancer.csv'\n",
    "data = pd.read_csv(data_url, header=None, names=header_list, index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>radius_sd_error</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>texture_sd_error</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>perimeter_sd_error</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>...</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>concave_points_sd_error</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>symmetry_sd_error</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>fractal_dimension_sd_error</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.00</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0000</td>\n",
       "      <td>569.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>456.00</td>\n",
       "      <td>479.00</td>\n",
       "      <td>522.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>474.0000</td>\n",
       "      <td>537.0000</td>\n",
       "      <td>537.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>432.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>457.00</td>\n",
       "      <td>511.00</td>\n",
       "      <td>514.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>411.0000</td>\n",
       "      <td>529.0000</td>\n",
       "      <td>539.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>535.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>B</td>\n",
       "      <td>12.34</td>\n",
       "      <td>18.22</td>\n",
       "      <td>134.7</td>\n",
       "      <td>512.2</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>...</td>\n",
       "      <td>12.36</td>\n",
       "      <td>27.26</td>\n",
       "      <td>101.7</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.07427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>357</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagnosis  radius_mean  radius_sd_error  radius_worst  texture_mean  \\\n",
       "count        569       569.00           569.00         569.0         569.0   \n",
       "unique         2       456.00           479.00         522.0         539.0   \n",
       "top            B        12.34            18.22         134.7         512.2   \n",
       "freq         357         4.00             3.00           3.0           3.0   \n",
       "\n",
       "        texture_sd_error  texture_worst  perimeter_mean  perimeter_sd_error  \\\n",
       "count           569.0000       569.0000           569.0               569.0   \n",
       "unique          474.0000       537.0000           537.0               542.0   \n",
       "top               0.1007         0.1147             0.0                 0.0   \n",
       "freq              5.0000         3.0000            13.0                13.0   \n",
       "\n",
       "        perimeter_worst           ...             concavity_worst  \\\n",
       "count          569.0000           ...                      569.00   \n",
       "unique         432.0000           ...                      457.00   \n",
       "top              0.1714           ...                       12.36   \n",
       "freq             4.0000           ...                        5.00   \n",
       "\n",
       "        concave_points_mean  concave_points_sd_error  concave_points_worst  \\\n",
       "count                569.00                    569.0                 569.0   \n",
       "unique               511.00                    514.0                 544.0   \n",
       "top                   27.26                    101.7                 826.4   \n",
       "freq                   3.00                      3.0                   2.0   \n",
       "\n",
       "        symmetry_mean  symmetry_sd_error  symmetry_worst  \\\n",
       "count        569.0000           569.0000           569.0   \n",
       "unique       411.0000           529.0000           539.0   \n",
       "top            0.1216             0.1486             0.0   \n",
       "freq           4.0000             3.0000            13.0   \n",
       "\n",
       "        fractal_dimension_mean  fractal_dimension_sd_error  \\\n",
       "count                    569.0                    569.0000   \n",
       "unique                   492.0                    500.0000   \n",
       "top                        0.0                      0.2383   \n",
       "freq                      13.0                      3.0000   \n",
       "\n",
       "        fractal_dimension_worst  \n",
       "count                 569.00000  \n",
       "unique                535.00000  \n",
       "top                     0.07427  \n",
       "freq                    3.00000  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment on any steps you might take to evaluate or transform the dataset.\n",
    "data.apply(pd.Categorical).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">smoothness_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">smoothness_sd_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">smoothness_worst</th>\n",
       "      <th colspan=\"2\" halign=\"left\">compactness_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">compactness_sd_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">compactness_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>2.000321</td>\n",
       "      <td>1.8510</td>\n",
       "      <td>21.135148</td>\n",
       "      <td>19.630</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.021438</td>\n",
       "      <td>0.01631</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>0.009061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>4.323929</td>\n",
       "      <td>3.6795</td>\n",
       "      <td>72.672406</td>\n",
       "      <td>58.455</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.02859</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.014205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          smoothness_mean         smoothness_sd_error          \\\n",
       "                     mean  median                mean  median   \n",
       "diagnosis                                                       \n",
       "B                2.000321  1.8510           21.135148  19.630   \n",
       "M                4.323929  3.6795           72.672406  58.455   \n",
       "\n",
       "          smoothness_worst           compactness_mean           \\\n",
       "                      mean    median             mean   median   \n",
       "diagnosis                                                        \n",
       "B                 0.007196  0.006530         0.021438  0.01631   \n",
       "M                 0.006780  0.006209         0.032281  0.02859   \n",
       "\n",
       "          compactness_sd_error           compactness_worst            \n",
       "                          mean    median              mean    median  \n",
       "diagnosis                                                             \n",
       "B                     0.025997  0.018400          0.009858  0.009061  \n",
       "M                     0.041824  0.037125          0.015060  0.014205  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the mean and median smoothness and compactness for benign and malignant tumors\n",
    "data.filter(regex='smoothness|compactness|diagnosis').groupby('diagnosis').agg(['mean', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothness: t statitic = 15.934158 p value = 0.000000000\n",
      "compactness: t statitic = 7.297077 p value = 0.000000000\n"
     ]
    }
   ],
   "source": [
    "# Do the groups differ? Explain how you would identify this.\n",
    "grouped = data.groupby('diagnosis')\n",
    "malignant = data.loc[grouped.groups['M']]\n",
    "benign = data.loc[grouped.groups['B']]\n",
    "\n",
    "t, p = ttest_ind(malignant['smoothness_mean'], benign['smoothness_mean']) # T-test for the means of two independent samples.\n",
    "print('{}: t statitic = {:.6f} p value = {:.9f}'.format('smoothness', t, p))\n",
    "\n",
    "t, p = ttest_ind(malignant['compactness_mean'], benign['compactness_mean']) # T-test for the means of two independent samples.\n",
    "print('{}: t statitic = {:.6f} p value = {:.9f}'.format('compactness', t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the cancer data by diagnosis, and summarizing by smoothness and compactness features, we see a discernable difference in measures of central tendancy (e.g. mean and median). To determine if this difference is statistically significant, we conduct a two-sided T-test with a null hypothesis that the two independent samples are identically distributed.\n",
    "\n",
    "Results indicate that we can reject the null hypothesis that mean cell nuclei smoothness for benign and malignant tupors are the same, based on a p_value approaching zero. Similarly for compactness, we can reject the null hypthoses that benign and malignant cell nuclei compactness are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to generate bootstrap samples of the data.\n",
    "def create_bootstrap_samples_from_dataframe(dataframe, n_samples=1000):\n",
    "    assert isinstance(dataframe, pd.core.frame.DataFrame), 'input data must be a pandas dataframe'\n",
    "    return dataframe.sample(n=n_samples, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original smoothness for benign cells: 2.000321\n",
      "resampled smoothness for benign cells: 2.006036\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "bootstrap_data = create_bootstrap_samples_from_dataframe(data, n_samples=10000)\n",
    "\n",
    "print('original smoothness for benign cells: {:.6f}'.format(data[data['diagnosis']=='B']['smoothness_mean'].mean()))\n",
    "print('resampled smoothness for benign cells: {:.6f}'.format(bootstrap_data[bootstrap_data['diagnosis']=='B']['smoothness_mean'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "Identify 2-3 variables that are predictive of a malignant tumor.  \n",
    "Display the relationship visually and write 1-2 sentences explaining the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ecohen/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# divide data into predictors (X) and predictand (y)\n",
    "x_train = data.ix[:, data.columns != 'diagnosis']\n",
    "y_train = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column means:\n",
      "[  1.41272917e+01   1.92896485e+01   9.19690334e+01   6.54889104e+02\n",
      "   9.63602812e-02   1.04340984e-01   8.87993158e-02   4.89191459e-02\n",
      "   1.81161863e-01   6.27976098e-02   4.05172056e-01   1.21685343e+00\n",
      "   2.86605923e+00   4.03370791e+01   7.04097891e-03   2.54781388e-02\n",
      "   3.18937163e-02   1.17961371e-02   2.05422988e-02   3.79490387e-03\n",
      "   1.62691898e+01   2.56772232e+01   1.07261213e+02   8.80583128e+02\n",
      "   1.32368594e-01   2.54265044e-01   2.72188483e-01   1.14606223e-01\n",
      "   2.90075571e-01   8.39458172e-02]\n",
      "column stds:\n",
      "[  3.52095076e+00   4.29725464e+00   2.42776193e+01   3.51604754e+02\n",
      "   1.40517641e-02   5.27663291e-02   7.96497253e-02   3.87687325e-02\n",
      "   2.73901809e-02   7.05415588e-03   2.77068942e-01   5.51163427e-01\n",
      "   2.02007710e+00   4.54510134e+01   2.99987837e-03   1.78924359e-02\n",
      "   3.01595231e-02   6.16486075e-03   8.25910439e-03   2.64374475e-03\n",
      "   4.82899258e+00   6.14085432e+00   3.35730016e+01   5.68856459e+02\n",
      "   2.28123569e-02   1.57198171e-01   2.08440875e-01   6.56745545e-02\n",
      "   6.18130785e-02   1.80453893e-02]\n",
      "these should be zero:\n",
      "[ -1.37363271e-16   6.86816353e-17  -1.24875700e-16  -2.18532476e-16\n",
      "   1.74825981e-16   1.99801121e-16   3.74627101e-17  -3.74627101e-17\n",
      "   1.87313551e-16   4.52674414e-16   2.49751401e-16  -1.03022453e-16\n",
      "  -3.49651961e-16  -1.31119486e-16   4.40186844e-16   1.81069766e-16\n",
      "   1.62338411e-16   1.24875700e-17   8.11692053e-17   6.24378502e-18\n",
      "  -8.24179623e-16   1.24875700e-17  -3.74627101e-16   0.00000000e+00\n",
      "  -2.37263831e-16  -3.37164391e-16   7.49254203e-17   2.24776261e-16\n",
      "   2.74726541e-16   2.12288691e-16]\n",
      "these should be one:\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# preprocess data to give each feature a mean of 0 and a std dev of 1. \n",
    "# This will help us later to determine which features are more important than others.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "print('column means:')\n",
    "print(scaler.mean_)\n",
    "print('column stds:')\n",
    "print(scaler.scale_)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "print('these should be zero:')\n",
    "print(x_train_scaled.mean(axis=0))\n",
    "print('these should be one:')\n",
    "print(x_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 23 candidates, totalling 230 fits\n",
      "BEST: {'C': 0.3} -0.096947666635038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 230 out of 230 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Next use ridge regression, choosing the hyperparameter based on 10-fold cross-validation.\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "classifier = linear_model.LogisticRegression(penalty='l1', class_weight='balanced')\n",
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 1.0, 3.0, 5.0, \n",
    "                    7.0, 10.0, 20.0, 500.0, 550., 600.0, 700., 800., 1000.0, 1100.0, 1200., 1500.]}\n",
    "\n",
    "cv = GridSearchCV(classifier,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='neg_log_loss',\n",
    "                  cv=kfold,\n",
    "                  verbose=1)\n",
    "\n",
    "cv.fit(x_train_scaled, y_train.values)\n",
    "print(\"BEST: {} {}\".format(cv.best_params_, cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.30015881e-07,   9.99999470e-01],\n",
       "       [  5.45969495e-04,   9.99454031e-01],\n",
       "       [  1.58713990e-05,   9.99984129e-01],\n",
       "       ..., \n",
       "       [  2.50526301e-02,   9.74947370e-01],\n",
       "       [  3.46357522e-08,   9.99999965e-01],\n",
       "       [  9.99601826e-01,   3.98173782e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_classifier = cv.best_estimator_\n",
    "final_classifier.fit(x_train_scaled, y_train.values)\n",
    "coefficients = final_classifier.coef_\n",
    "intercept = final_classifier.intercept_\n",
    "final_classifier.predict_proba(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[data['diagnosis']=='M'].plot(kind='density', subplots=True, layout=(10,3), sharex=False, figsize=(18,18))\n",
    "data[data['diagnosis']=='B'].plot(kind='density', subplots=True, layout=(10,3), sharex=False, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.predict([[0, 0], [4, 4]]))\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "from scipy.special import stdtr\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create sample data.\n",
    "a = np.random.randn(40)\n",
    "b = 4*np.random.randn(50)\n",
    "\n",
    "# Use scipy.stats.ttest_ind.\n",
    "t, p = ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind:            t = %g  p = %g\" % (t, p))\n",
    "\n",
    "# Compute the descriptive statistics of a and b.\n",
    "abar = a.mean()\n",
    "avar = a.var(ddof=1)\n",
    "na = a.size\n",
    "adof = na - 1\n",
    "\n",
    "bbar = b.mean()\n",
    "bvar = b.var(ddof=1)\n",
    "nb = b.size\n",
    "bdof = nb - 1\n",
    "\n",
    "# Use scipy.stats.ttest_ind_from_stats.\n",
    "t2, p2 = ttest_ind_from_stats(abar, np.sqrt(avar), na,\n",
    "                              bbar, np.sqrt(bvar), nb,\n",
    "                              equal_var=False)\n",
    "print(\"ttest_ind_from_stats: t = %g  p = %g\" % (t2, p2))\n",
    "\n",
    "# Use the formulas directly.\n",
    "tf = (abar - bbar) / np.sqrt(avar/na + bvar/nb)\n",
    "dof = (avar/na + bvar/nb)**2 / (avar**2/(na**2*adof) + bvar**2/(nb**2*bdof))\n",
    "pf = 2*stdtr(dof, -np.abs(tf))\n",
    "\n",
    "print(\"formula:              t = %g  p = %g\" % (tf, pf))"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
